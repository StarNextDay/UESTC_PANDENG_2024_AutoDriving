Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='Ours data', data_path='ETTh1.csv', dec_in=7, depth=4, des='test', devices='0,1,2,3', dim=128, distil=True, dropout=0.1, e_layers=2, embed='timeF', enc_in=7, epochs=500, eval_save_frq=10, factor=1, features='M', freq='h', gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=1e-05, loss='MSE', lradj='type1', mask_rate=0.25, model='iTransformer', model_id='test', model_name='Baseline_concat_mlp', moving_avg=25, n_heads=8, num_class=4, num_kernels=6, num_workers=10, output_attention=True, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=3, pred_len=96, root_path='./data/ETT/', seasonal_patterns='Monthly', seq_len=300, target='OT', task_name='classification', tasks=[3, 3, 2, 4], top_k=5, train_epochs=10, use_amp=False, use_gpu=False, use_multi_gpu=False)

 <class 'layers.Model.Baseline_concat_mlp'>
Baseline_concat_mlp(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x Concat_Mlp(
      (tb): ModuleDict(
        (0): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (1): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (2): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (3): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (cb): Conv1dMlp(
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (act): GELU(approximate='none')
        (conv2): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
        (drop): Dropout(p=0.0, inplace=False)
      )
      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
)
2024-07-01 07:12:03	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [010/500] Loss: 1.2541 Acc: [0.641, 0.555, 0.77, 0.373]
2024-07-01 07:12:21	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [020/500] Loss: 1.1166 Acc: [0.684, 0.617, 0.77, 0.397]
2024-07-01 07:12:40	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [030/500] Loss: 0.9838 Acc: [0.689, 0.722, 0.77, 0.502]
2024-07-01 07:12:58	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [040/500] Loss: 0.8647 Acc: [0.727, 0.727, 0.761, 0.507]
2024-07-01 07:13:17	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [050/500] Loss: 0.8315 Acc: [0.699, 0.718, 0.722, 0.531]
2024-07-01 07:13:35	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [060/500] Loss: 0.8430 Acc: [0.636, 0.761, 0.746, 0.627]
2024-07-01 07:13:54	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [070/500] Loss: 0.5657 Acc: [0.699, 0.746, 0.555, 0.569]
2024-07-01 07:14:12	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [080/500] Loss: 0.5187 Acc: [0.651, 0.756, 0.632, 0.636]
2024-07-01 07:14:31	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [090/500] Loss: 0.4527 Acc: [0.675, 0.746, 0.766, 0.646]
2024-07-01 07:14:49	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [100/500] Loss: 0.3318 Acc: [0.699, 0.742, 0.718, 0.675]
2024-07-01 07:15:08	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [110/500] Loss: 0.2584 Acc: [0.699, 0.708, 0.727, 0.713]
2024-07-01 07:15:26	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [120/500] Loss: 0.1918 Acc: [0.78, 0.727, 0.636, 0.699]
2024-07-01 07:15:45	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [130/500] Loss: 0.1296 Acc: [0.785, 0.722, 0.737, 0.751]
2024-07-01 07:16:03	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [140/500] Loss: 0.0911 Acc: [0.699, 0.66, 0.794, 0.718]
2024-07-01 07:16:22	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [150/500] Loss: 0.0470 Acc: [0.77, 0.775, 0.727, 0.761]
2024-07-01 07:16:41	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [160/500] Loss: 0.0241 Acc: [0.78, 0.775, 0.775, 0.746]
2024-07-01 07:16:59	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [170/500] Loss: 0.0165 Acc: [0.746, 0.775, 0.794, 0.766]
2024-07-01 07:17:18	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [180/500] Loss: 0.0070 Acc: [0.766, 0.785, 0.78, 0.775]
2024-07-01 07:17:36	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [190/500] Loss: 0.0119 Acc: [0.766, 0.828, 0.732, 0.565]
2024-07-01 07:17:55	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [200/500] Loss: 0.0077 Acc: [0.804, 0.823, 0.799, 0.785]
2024-07-01 07:18:14	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [210/500] Loss: 0.0023 Acc: [0.809, 0.852, 0.818, 0.789]
2024-07-01 07:18:32	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [220/500] Loss: 0.0055 Acc: [0.804, 0.861, 0.789, 0.789]
2024-07-01 07:18:51	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [230/500] Loss: 0.0011 Acc: [0.699, 0.746, 0.785, 0.761]
2024-07-01 07:19:09	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [240/500] Loss: 0.0021 Acc: [0.842, 0.914, 0.828, 0.799]
2024-07-01 07:19:28	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [250/500] Loss: 0.0004 Acc: [0.856, 0.89, 0.799, 0.799]
2024-07-01 07:19:46	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [260/500] Loss: 0.0131 Acc: [0.813, 0.904, 0.833, 0.78]
2024-07-01 07:20:05	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [270/500] Loss: 0.0013 Acc: [0.861, 0.89, 0.809, 0.799]
2024-07-01 07:20:23	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [280/500] Loss: 0.0002 Acc: [0.885, 0.9, 0.789, 0.804]
2024-07-01 07:20:42	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [290/500] Loss: 0.0018 Acc: [0.895, 0.871, 0.813, 0.828]
2024-07-01 07:21:01	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [300/500] Loss: 0.0007 Acc: [0.852, 0.943, 0.813, 0.77]
2024-07-01 07:21:19	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [310/500] Loss: 0.0004 Acc: [0.885, 0.919, 0.794, 0.775]
2024-07-01 07:21:38	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [320/500] Loss: 0.0000 Acc: [0.871, 0.928, 0.818, 0.799]
2024-07-01 07:21:56	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [330/500] Loss: 0.0001 Acc: [0.914, 0.9, 0.823, 0.828]
2024-07-01 07:22:15	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [340/500] Loss: 0.0026 Acc: [0.909, 0.938, 0.856, 0.828]
2024-07-01 07:22:33	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [350/500] Loss: 0.0000 Acc: [0.914, 0.909, 0.842, 0.828]
2024-07-01 07:22:52	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [360/500] Loss: 0.0001 Acc: [0.895, 0.947, 0.766, 0.813]
2024-07-01 07:23:10	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [370/500] Loss: 0.0004 Acc: [0.89, 0.933, 0.847, 0.766]
2024-07-01 07:23:29	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [380/500] Loss: 0.0000 Acc: [0.9, 0.933, 0.852, 0.809]
2024-07-01 07:23:47	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [390/500] Loss: 0.0005 Acc: [0.909, 0.919, 0.871, 0.775]
2024-07-01 07:24:06	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [400/500] Loss: 0.0001 Acc: [0.9, 0.952, 0.876, 0.828]
2024-07-01 07:24:24	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [410/500] Loss: 0.0000 Acc: [0.909, 0.943, 0.871, 0.818]
2024-07-01 07:24:43	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [420/500] Loss: 0.0000 Acc: [0.9, 0.943, 0.89, 0.823]
2024-07-01 07:25:01	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [430/500] Loss: 0.0000 Acc: [0.923, 0.957, 0.828, 0.828]
2024-07-01 07:25:20	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [440/500] Loss: 0.0000 Acc: [0.919, 0.962, 0.861, 0.813]
2024-07-01 07:25:38	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [450/500] Loss: 0.0019 Acc: [0.885, 0.957, 0.885, 0.799]
2024-07-01 07:25:57	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [460/500] Loss: 0.0000 Acc: [0.914, 0.952, 0.852, 0.818]
2024-07-01 07:26:15	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [470/500] Loss: 0.0000 Acc: [0.9, 0.962, 0.852, 0.833]
2024-07-01 07:26:34	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [480/500] Loss: 0.0000 Acc: [0.885, 0.947, 0.904, 0.818]
2024-07-01 07:26:53	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [490/500] Loss: 0.0002 Acc: [0.904, 0.962, 0.871, 0.828]
2024-07-01 07:27:11	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [0] Epoch [500/500] Loss: 0.0000 Acc: [0.904, 0.957, 0.885, 0.823]
mean acc: 0.73818177
mean f1: 0.725178539787968
Baseline_concat_mlp(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x Concat_Mlp(
      (tb): ModuleDict(
        (0): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (1): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (2): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (3): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (cb): Conv1dMlp(
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (act): GELU(approximate='none')
        (conv2): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
        (drop): Dropout(p=0.0, inplace=False)
      )
      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
)
2024-07-01 07:27:31	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [010/500] Loss: 1.2703 Acc: [0.651, 0.574, 0.732, 0.368]
2024-07-01 07:27:49	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [020/500] Loss: 1.1635 Acc: [0.67, 0.565, 0.732, 0.407]
2024-07-01 07:28:08	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [030/500] Loss: 1.0534 Acc: [0.67, 0.656, 0.732, 0.411]
2024-07-01 07:28:27	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [040/500] Loss: 0.9631 Acc: [0.67, 0.694, 0.751, 0.526]
2024-07-01 07:28:45	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [050/500] Loss: 0.9115 Acc: [0.67, 0.742, 0.732, 0.574]
2024-07-01 07:29:04	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [060/500] Loss: 0.8209 Acc: [0.694, 0.766, 0.751, 0.569]
2024-07-01 07:29:22	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [070/500] Loss: 0.6514 Acc: [0.742, 0.756, 0.502, 0.617]
2024-07-01 07:29:41	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [080/500] Loss: 0.5798 Acc: [0.751, 0.775, 0.584, 0.684]
2024-07-01 07:30:00	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [090/500] Loss: 0.3742 Acc: [0.742, 0.818, 0.699, 0.656]
2024-07-01 07:30:18	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [100/500] Loss: 0.1508 Acc: [0.694, 0.689, 0.545, 0.732]
2024-07-01 07:30:37	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [110/500] Loss: 0.1471 Acc: [0.713, 0.766, 0.703, 0.737]
2024-07-01 07:30:55	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [120/500] Loss: 0.1183 Acc: [0.737, 0.761, 0.708, 0.756]
2024-07-01 07:31:14	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [130/500] Loss: 0.1054 Acc: [0.77, 0.833, 0.722, 0.761]
2024-07-01 07:31:32	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [140/500] Loss: 0.0735 Acc: [0.856, 0.794, 0.737, 0.751]
2024-07-01 07:31:51	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [150/500] Loss: 0.0831 Acc: [0.78, 0.809, 0.713, 0.77]
2024-07-01 07:32:10	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [160/500] Loss: 0.0291 Acc: [0.847, 0.809, 0.742, 0.766]
2024-07-01 07:32:28	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [170/500] Loss: 0.0316 Acc: [0.809, 0.856, 0.746, 0.775]
2024-07-01 07:32:47	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [180/500] Loss: 0.0130 Acc: [0.809, 0.828, 0.746, 0.785]
2024-07-01 07:33:05	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [190/500] Loss: 0.0123 Acc: [0.823, 0.909, 0.833, 0.789]
2024-07-01 07:33:24	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [200/500] Loss: 0.0099 Acc: [0.722, 0.847, 0.756, 0.746]
2024-07-01 07:33:42	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [210/500] Loss: 0.0022 Acc: [0.818, 0.809, 0.823, 0.804]
2024-07-01 07:34:01	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [220/500] Loss: 0.0316 Acc: [0.804, 0.914, 0.813, 0.818]
2024-07-01 07:34:19	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [230/500] Loss: 0.0014 Acc: [0.813, 0.842, 0.823, 0.818]
2024-07-01 07:34:38	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [240/500] Loss: 0.0024 Acc: [0.833, 0.852, 0.847, 0.833]
2024-07-01 07:34:56	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [250/500] Loss: 0.0003 Acc: [0.852, 0.833, 0.861, 0.842]
2024-07-01 07:35:15	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [260/500] Loss: 0.0033 Acc: [0.904, 0.809, 0.828, 0.852]
2024-07-01 07:35:33	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [270/500] Loss: 0.0003 Acc: [0.833, 0.813, 0.813, 0.847]
2024-07-01 07:35:52	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [280/500] Loss: 0.0005 Acc: [0.88, 0.842, 0.813, 0.837]
2024-07-01 07:36:10	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [290/500] Loss: 0.0013 Acc: [0.861, 0.823, 0.823, 0.885]
2024-07-01 07:36:29	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [300/500] Loss: 0.0001 Acc: [0.847, 0.909, 0.818, 0.866]
2024-07-01 07:36:47	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [310/500] Loss: 0.0001 Acc: [0.861, 0.89, 0.818, 0.885]
2024-07-01 07:37:06	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [320/500] Loss: 0.0005 Acc: [0.833, 0.89, 0.861, 0.861]
2024-07-01 07:37:24	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [330/500] Loss: 0.0017 Acc: [0.847, 0.909, 0.823, 0.861]
2024-07-01 07:37:43	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [340/500] Loss: 0.0008 Acc: [0.89, 0.861, 0.828, 0.856]
2024-07-01 07:38:01	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [350/500] Loss: 0.0002 Acc: [0.847, 0.919, 0.789, 0.837]
2024-07-01 07:38:20	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [360/500] Loss: 0.0001 Acc: [0.885, 0.904, 0.789, 0.876]
2024-07-01 07:38:38	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [370/500] Loss: 0.0017 Acc: [0.751, 0.804, 0.885, 0.785]
2024-07-01 07:38:57	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [380/500] Loss: 0.0004 Acc: [0.876, 0.914, 0.852, 0.833]
2024-07-01 07:39:15	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [390/500] Loss: 0.0004 Acc: [0.885, 0.919, 0.828, 0.856]
2024-07-01 07:39:34	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [400/500] Loss: 0.0001 Acc: [0.876, 0.919, 0.828, 0.885]
2024-07-01 07:39:53	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [410/500] Loss: 0.0000 Acc: [0.871, 0.919, 0.818, 0.876]
2024-07-01 07:40:11	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [420/500] Loss: 0.0001 Acc: [0.9, 0.933, 0.842, 0.895]
2024-07-01 07:40:30	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [430/500] Loss: 0.0001 Acc: [0.876, 0.919, 0.833, 0.909]
2024-07-01 07:40:48	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [440/500] Loss: 0.0001 Acc: [0.789, 0.895, 0.871, 0.88]
2024-07-01 07:41:07	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [450/500] Loss: 0.0001 Acc: [0.809, 0.837, 0.799, 0.914]
2024-07-01 07:41:26	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [460/500] Loss: 0.0002 Acc: [0.837, 0.928, 0.842, 0.919]
2024-07-01 07:41:44	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [470/500] Loss: 0.0001 Acc: [0.871, 0.928, 0.818, 0.9]
2024-07-01 07:42:03	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [480/500] Loss: 0.0171 Acc: [0.919, 0.833, 0.871, 0.914]
2024-07-01 07:42:21	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [490/500] Loss: 0.0001 Acc: [0.919, 0.933, 0.833, 0.904]
2024-07-01 07:42:40	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [1] Epoch [500/500] Loss: 0.0000 Acc: [0.833, 0.919, 0.856, 0.914]
mean acc: 0.7605263
mean f1: 0.7530569468351278
Baseline_concat_mlp(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x Concat_Mlp(
      (tb): ModuleDict(
        (0): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (1): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (2): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (3): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (cb): Conv1dMlp(
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (act): GELU(approximate='none')
        (conv2): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
        (drop): Dropout(p=0.0, inplace=False)
      )
      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
)
2024-07-01 07:42:59	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [010/500] Loss: 1.2337 Acc: [0.636, 0.632, 0.766, 0.33]
2024-07-01 07:43:18	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [020/500] Loss: 1.1030 Acc: [0.684, 0.612, 0.766, 0.383]
2024-07-01 07:43:37	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [030/500] Loss: 1.0144 Acc: [0.679, 0.694, 0.775, 0.407]
2024-07-01 07:43:55	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [040/500] Loss: 0.9151 Acc: [0.665, 0.684, 0.746, 0.531]
2024-07-01 07:44:14	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [050/500] Loss: 0.8619 Acc: [0.727, 0.718, 0.684, 0.603]
2024-07-01 07:44:32	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [060/500] Loss: 0.8247 Acc: [0.684, 0.679, 0.531, 0.565]
2024-07-01 07:44:51	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [070/500] Loss: 0.7618 Acc: [0.612, 0.77, 0.708, 0.651]
2024-07-01 07:45:09	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [080/500] Loss: 0.6549 Acc: [0.746, 0.751, 0.507, 0.617]
2024-07-01 07:45:28	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [090/500] Loss: 0.5502 Acc: [0.766, 0.713, 0.536, 0.584]
2024-07-01 07:45:47	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [100/500] Loss: 0.3438 Acc: [0.718, 0.78, 0.703, 0.665]
2024-07-01 07:46:05	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [110/500] Loss: 0.2420 Acc: [0.78, 0.708, 0.589, 0.703]
2024-07-01 07:46:24	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [120/500] Loss: 0.1821 Acc: [0.78, 0.789, 0.78, 0.699]
2024-07-01 07:46:42	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [130/500] Loss: 0.1032 Acc: [0.756, 0.837, 0.761, 0.699]
2024-07-01 07:47:01	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [140/500] Loss: 0.0849 Acc: [0.722, 0.833, 0.718, 0.742]
2024-07-01 07:47:20	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [150/500] Loss: 0.0470 Acc: [0.809, 0.852, 0.837, 0.742]
2024-07-01 07:47:38	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [160/500] Loss: 0.0410 Acc: [0.794, 0.856, 0.761, 0.775]
2024-07-01 07:47:57	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [170/500] Loss: 0.0228 Acc: [0.775, 0.823, 0.746, 0.756]
2024-07-01 07:48:16	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [180/500] Loss: 0.0110 Acc: [0.813, 0.856, 0.737, 0.809]
2024-07-01 07:48:34	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [190/500] Loss: 0.0148 Acc: [0.871, 0.809, 0.703, 0.766]
2024-07-01 07:48:53	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [200/500] Loss: 0.0087 Acc: [0.89, 0.876, 0.785, 0.77]
2024-07-01 07:49:11	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [210/500] Loss: 0.0025 Acc: [0.88, 0.833, 0.694, 0.799]
2024-07-01 07:49:30	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [220/500] Loss: 0.0036 Acc: [0.833, 0.828, 0.694, 0.804]
2024-07-01 07:49:49	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [230/500] Loss: 0.0079 Acc: [0.89, 0.823, 0.689, 0.799]
2024-07-01 07:50:07	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [240/500] Loss: 0.0019 Acc: [0.885, 0.823, 0.722, 0.823]
2024-07-01 07:50:26	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [250/500] Loss: 0.0215 Acc: [0.89, 0.885, 0.756, 0.828]
2024-07-01 07:50:44	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [260/500] Loss: 0.0004 Acc: [0.885, 0.785, 0.722, 0.828]
2024-07-01 07:51:03	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [270/500] Loss: 0.0003 Acc: [0.914, 0.923, 0.751, 0.823]
2024-07-01 07:51:21	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [280/500] Loss: 0.0004 Acc: [0.895, 0.828, 0.718, 0.837]
2024-07-01 07:51:40	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [290/500] Loss: 0.0002 Acc: [0.923, 0.938, 0.746, 0.833]
2024-07-01 07:51:58	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [300/500] Loss: 0.0006 Acc: [0.818, 0.871, 0.713, 0.856]
2024-07-01 07:52:17	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [310/500] Loss: 0.0004 Acc: [0.938, 0.933, 0.746, 0.847]
2024-07-01 07:52:35	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [320/500] Loss: 0.0000 Acc: [0.885, 0.909, 0.828, 0.833]
2024-07-01 07:52:54	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [330/500] Loss: 0.0027 Acc: [0.928, 0.895, 0.818, 0.852]
2024-07-01 07:53:12	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [340/500] Loss: 0.0002 Acc: [0.909, 0.871, 0.77, 0.861]
2024-07-01 07:53:31	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [350/500] Loss: 0.0002 Acc: [0.938, 0.88, 0.746, 0.861]
2024-07-01 07:53:50	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [360/500] Loss: 0.0001 Acc: [0.909, 0.923, 0.794, 0.837]
2024-07-01 07:54:08	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [370/500] Loss: 0.0001 Acc: [0.895, 0.9, 0.718, 0.871]
2024-07-01 07:54:27	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [380/500] Loss: 0.0008 Acc: [0.904, 0.923, 0.794, 0.856]
2024-07-01 07:54:46	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [390/500] Loss: 0.0004 Acc: [0.909, 0.909, 0.794, 0.852]
2024-07-01 07:55:04	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [400/500] Loss: 0.0002 Acc: [0.904, 0.933, 0.794, 0.852]
2024-07-01 07:55:23	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [410/500] Loss: 0.0001 Acc: [0.914, 0.943, 0.809, 0.852]
2024-07-01 07:55:41	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [420/500] Loss: 0.0000 Acc: [0.904, 0.923, 0.804, 0.866]
2024-07-01 07:56:00	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [430/500] Loss: 0.0000 Acc: [0.919, 0.933, 0.809, 0.861]
2024-07-01 07:56:18	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [440/500] Loss: 0.0008 Acc: [0.909, 0.933, 0.813, 0.866]
2024-07-01 07:56:37	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [450/500] Loss: 0.0001 Acc: [0.904, 0.923, 0.823, 0.876]
2024-07-01 07:56:56	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [460/500] Loss: 0.0000 Acc: [0.909, 0.923, 0.823, 0.866]
2024-07-01 07:57:14	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [470/500] Loss: 0.0000 Acc: [0.967, 0.842, 0.952, 0.88]
2024-07-01 07:57:33	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [480/500] Loss: 0.0000 Acc: [0.871, 0.923, 0.818, 0.861]
2024-07-01 07:57:51	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [490/500] Loss: 0.0011 Acc: [0.904, 0.885, 0.799, 0.876]
2024-07-01 07:58:10	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [2] Epoch [500/500] Loss: 0.0000 Acc: [0.952, 0.923, 0.904, 0.861]
mean acc: 0.76175433
mean f1: 0.7543015873831299
Baseline_concat_mlp(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x Concat_Mlp(
      (tb): ModuleDict(
        (0): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (1): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (2): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (3): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (cb): Conv1dMlp(
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (act): GELU(approximate='none')
        (conv2): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
        (drop): Dropout(p=0.0, inplace=False)
      )
      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
)
2024-07-01 07:58:29	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [010/500] Loss: 1.2575 Acc: [0.627, 0.622, 0.766, 0.297]
2024-07-01 07:58:48	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [020/500] Loss: 1.1672 Acc: [0.708, 0.55, 0.766, 0.397]
2024-07-01 07:59:07	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [030/500] Loss: 1.0929 Acc: [0.708, 0.636, 0.766, 0.411]
2024-07-01 07:59:25	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [040/500] Loss: 1.0235 Acc: [0.708, 0.651, 0.761, 0.407]
2024-07-01 07:59:44	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [050/500] Loss: 1.0013 Acc: [0.699, 0.656, 0.656, 0.569]
2024-07-01 08:00:03	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [060/500] Loss: 0.8977 Acc: [0.689, 0.785, 0.574, 0.598]
2024-07-01 08:00:21	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [070/500] Loss: 0.7374 Acc: [0.718, 0.722, 0.431, 0.545]
2024-07-01 08:00:40	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [080/500] Loss: 0.6920 Acc: [0.708, 0.766, 0.488, 0.574]
2024-07-01 08:00:58	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [090/500] Loss: 0.5757 Acc: [0.708, 0.756, 0.541, 0.622]
2024-07-01 08:01:17	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [100/500] Loss: 0.5231 Acc: [0.718, 0.828, 0.627, 0.608]
2024-07-01 08:01:36	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [110/500] Loss: 0.3673 Acc: [0.694, 0.708, 0.421, 0.584]
2024-07-01 08:01:54	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [120/500] Loss: 0.3780 Acc: [0.679, 0.794, 0.435, 0.66]
2024-07-01 08:02:13	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [130/500] Loss: 0.2569 Acc: [0.67, 0.78, 0.431, 0.66]
2024-07-01 08:02:31	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [140/500] Loss: 0.1969 Acc: [0.703, 0.77, 0.502, 0.665]
2024-07-01 08:02:50	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [150/500] Loss: 0.1489 Acc: [0.727, 0.789, 0.512, 0.675]
2024-07-01 08:03:08	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [160/500] Loss: 0.2398 Acc: [0.761, 0.751, 0.636, 0.713]
2024-07-01 08:03:27	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [170/500] Loss: 0.1500 Acc: [0.756, 0.761, 0.498, 0.679]
2024-07-01 08:03:45	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [180/500] Loss: 0.2046 Acc: [0.789, 0.823, 0.699, 0.732]
2024-07-01 08:04:04	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [190/500] Loss: 0.1124 Acc: [0.761, 0.794, 0.67, 0.737]
2024-07-01 08:04:22	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [200/500] Loss: 0.2159 Acc: [0.818, 0.794, 0.708, 0.756]
2024-07-01 08:04:41	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [210/500] Loss: 0.0732 Acc: [0.809, 0.794, 0.665, 0.761]
2024-07-01 08:04:59	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [220/500] Loss: 0.0375 Acc: [0.828, 0.842, 0.679, 0.775]
2024-07-01 08:05:18	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [230/500] Loss: 0.0351 Acc: [0.828, 0.818, 0.689, 0.766]
2024-07-01 08:05:37	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [240/500] Loss: 0.0981 Acc: [0.914, 0.775, 0.636, 0.689]
2024-07-01 08:05:55	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [250/500] Loss: 0.0254 Acc: [0.852, 0.852, 0.641, 0.804]
2024-07-01 08:06:14	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [260/500] Loss: 0.0108 Acc: [0.876, 0.856, 0.737, 0.809]
2024-07-01 08:06:32	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [270/500] Loss: 0.0102 Acc: [0.909, 0.852, 0.718, 0.809]
2024-07-01 08:06:51	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [280/500] Loss: 0.0071 Acc: [0.928, 0.871, 0.746, 0.818]
2024-07-01 08:07:09	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [290/500] Loss: 0.0054 Acc: [0.919, 0.89, 0.746, 0.809]
2024-07-01 08:07:28	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [300/500] Loss: 0.0028 Acc: [0.895, 0.837, 0.737, 0.818]
2024-07-01 08:07:46	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [310/500] Loss: 0.0010 Acc: [0.943, 0.88, 0.708, 0.828]
2024-07-01 08:08:05	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [320/500] Loss: 0.0007 Acc: [0.938, 0.904, 0.804, 0.813]
2024-07-01 08:08:24	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [330/500] Loss: 0.0003 Acc: [0.947, 0.852, 0.67, 0.856]
2024-07-01 08:08:42	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [340/500] Loss: 0.0012 Acc: [0.943, 0.895, 0.727, 0.823]
2024-07-01 08:09:01	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [350/500] Loss: 0.0001 Acc: [0.938, 0.876, 0.737, 0.847]
2024-07-01 08:09:19	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [360/500] Loss: 0.0003 Acc: [0.947, 0.885, 0.722, 0.861]
2024-07-01 08:09:38	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [370/500] Loss: 0.0001 Acc: [0.947, 0.871, 0.722, 0.852]
2024-07-01 08:09:56	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [380/500] Loss: 0.0619 Acc: [0.919, 0.847, 0.722, 0.761]
2024-07-01 08:10:15	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [390/500] Loss: 0.0010 Acc: [0.947, 0.914, 0.766, 0.871]
2024-07-01 08:10:34	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [400/500] Loss: 0.0000 Acc: [0.962, 0.88, 0.742, 0.847]
2024-07-01 08:10:52	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [410/500] Loss: 0.0002 Acc: [0.967, 0.895, 0.794, 0.89]
2024-07-01 08:11:11	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [420/500] Loss: 0.0000 Acc: [0.962, 0.885, 0.823, 0.876]
2024-07-01 08:11:29	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [430/500] Loss: 0.0000 Acc: [0.957, 0.919, 0.799, 0.866]
2024-07-01 08:11:48	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [440/500] Loss: 0.0003 Acc: [0.957, 0.904, 0.794, 0.833]
2024-07-01 08:12:06	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [450/500] Loss: 0.0001 Acc: [0.971, 0.885, 0.823, 0.866]
2024-07-01 08:12:25	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [460/500] Loss: 0.0001 Acc: [0.971, 0.904, 0.852, 0.89]
2024-07-01 08:12:43	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [470/500] Loss: 0.0000 Acc: [0.957, 0.88, 0.852, 0.88]
2024-07-01 08:13:02	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [480/500] Loss: 0.0000 Acc: [0.967, 0.909, 0.876, 0.876]
2024-07-01 08:13:21	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [490/500] Loss: 0.0000 Acc: [0.967, 0.909, 0.804, 0.895]
2024-07-01 08:13:39	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [3] Epoch [500/500] Loss: 0.0000 Acc: [0.981, 0.909, 0.833, 0.895]
mean acc: 0.75566983
mean f1: 0.746397287177012
Baseline_concat_mlp(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x Concat_Mlp(
      (tb): ModuleDict(
        (0): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (1): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (2): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
        (3): Conv1dMlp(
          (conv1): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,))
          (act): GELU(approximate='none')
          (conv2): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (cb): Conv1dMlp(
        (conv1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (act): GELU(approximate='none')
        (conv2): Conv1d(512, 128, kernel_size=(1,), stride=(1,))
        (drop): Dropout(p=0.0, inplace=False)
      )
      (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
  )
)
2024-07-01 08:13:59	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [010/500] Loss: 1.2468 Acc: [0.611, 0.683, 0.76, 0.394]
2024-07-01 08:14:17	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [020/500] Loss: 1.1091 Acc: [0.659, 0.635, 0.76, 0.418]
2024-07-01 08:14:36	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [030/500] Loss: 0.9733 Acc: [0.654, 0.649, 0.755, 0.5]
2024-07-01 08:14:55	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [040/500] Loss: 0.8518 Acc: [0.688, 0.635, 0.745, 0.538]
2024-07-01 08:15:13	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [050/500] Loss: 0.8348 Acc: [0.659, 0.697, 0.793, 0.514]
2024-07-01 08:15:32	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [060/500] Loss: 0.7549 Acc: [0.731, 0.755, 0.659, 0.591]
2024-07-01 08:15:50	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [070/500] Loss: 0.7174 Acc: [0.673, 0.784, 0.707, 0.62]
2024-07-01 08:16:09	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [080/500] Loss: 0.5428 Acc: [0.692, 0.721, 0.514, 0.577]
2024-07-01 08:16:27	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [090/500] Loss: 0.5152 Acc: [0.707, 0.793, 0.577, 0.596]
2024-07-01 08:16:46	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [100/500] Loss: 0.3929 Acc: [0.712, 0.817, 0.591, 0.663]
2024-07-01 08:17:05	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [110/500] Loss: 0.2158 Acc: [0.692, 0.808, 0.639, 0.692]
2024-07-01 08:17:23	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [120/500] Loss: 0.2020 Acc: [0.702, 0.837, 0.673, 0.678]
2024-07-01 08:17:42	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [130/500] Loss: 0.0711 Acc: [0.75, 0.827, 0.76, 0.712]
2024-07-01 08:18:00	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [140/500] Loss: 0.0553 Acc: [0.736, 0.822, 0.702, 0.721]
2024-07-01 08:18:19	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [150/500] Loss: 0.0339 Acc: [0.697, 0.822, 0.726, 0.76]
2024-07-01 08:18:37	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [160/500] Loss: 0.0305 Acc: [0.764, 0.798, 0.702, 0.793]
2024-07-01 08:18:56	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [170/500] Loss: 0.0446 Acc: [0.764, 0.861, 0.678, 0.76]
2024-07-01 08:19:14	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [180/500] Loss: 0.0049 Acc: [0.803, 0.865, 0.716, 0.788]
2024-07-01 08:19:33	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [190/500] Loss: 0.0042 Acc: [0.793, 0.87, 0.731, 0.793]
2024-07-01 08:19:51	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [200/500] Loss: 0.0022 Acc: [0.813, 0.889, 0.764, 0.822]
2024-07-01 08:20:10	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [210/500] Loss: 0.0029 Acc: [0.817, 0.798, 0.798, 0.827]
2024-07-01 08:20:29	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [220/500] Loss: 0.0008 Acc: [0.808, 0.779, 0.683, 0.808]
2024-07-01 08:20:47	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [230/500] Loss: 0.0008 Acc: [0.793, 0.861, 0.712, 0.769]
2024-07-01 08:21:06	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [240/500] Loss: 0.0005 Acc: [0.793, 0.923, 0.798, 0.779]
2024-07-01 08:21:25	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [250/500] Loss: 0.0018 Acc: [0.822, 0.88, 0.841, 0.837]
2024-07-01 08:21:43	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [260/500] Loss: 0.0005 Acc: [0.793, 0.861, 0.798, 0.827]
2024-07-01 08:22:02	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [270/500] Loss: 0.0006 Acc: [0.808, 0.846, 0.856, 0.817]
2024-07-01 08:22:20	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [280/500] Loss: 0.0008 Acc: [0.865, 0.942, 0.731, 0.798]
2024-07-01 08:22:39	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [290/500] Loss: 0.0009 Acc: [0.846, 0.947, 0.841, 0.817]
2024-07-01 08:22:58	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [300/500] Loss: 0.0001 Acc: [0.841, 0.865, 0.846, 0.827]
2024-07-01 08:23:16	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [310/500] Loss: 0.0002 Acc: [0.889, 0.947, 0.784, 0.832]
2024-07-01 08:23:35	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [320/500] Loss: 0.0009 Acc: [0.822, 0.87, 0.851, 0.808]
2024-07-01 08:23:54	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [330/500] Loss: 0.0000 Acc: [0.856, 0.933, 0.716, 0.745]
2024-07-01 08:24:12	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [340/500] Loss: 0.0003 Acc: [0.822, 0.942, 0.798, 0.808]
2024-07-01 08:24:31	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [350/500] Loss: 0.0001 Acc: [0.846, 0.938, 0.822, 0.793]
2024-07-01 08:24:49	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [360/500] Loss: 0.0004 Acc: [0.87, 0.909, 0.861, 0.817]
2024-07-01 08:25:08	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [370/500] Loss: 0.0000 Acc: [0.899, 0.942, 0.856, 0.808]
2024-07-01 08:25:27	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [380/500] Loss: 0.0000 Acc: [0.837, 0.947, 0.697, 0.793]
2024-07-01 08:25:45	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [390/500] Loss: 0.0001 Acc: [0.837, 0.962, 0.851, 0.813]
2024-07-01 08:26:04	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [400/500] Loss: 0.0002 Acc: [0.87, 0.947, 0.841, 0.817]
2024-07-01 08:26:22	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [410/500] Loss: 0.0000 Acc: [0.889, 0.957, 0.774, 0.798]
2024-07-01 08:26:41	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [420/500] Loss: 0.0003 Acc: [0.817, 0.76, 0.865, 0.784]
2024-07-01 08:26:59	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [430/500] Loss: 0.0000 Acc: [0.899, 0.952, 0.832, 0.817]
2024-07-01 08:27:18	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [440/500] Loss: 0.0002 Acc: [0.861, 0.962, 0.745, 0.822]
2024-07-01 08:27:37	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [450/500] Loss: 0.0000 Acc: [0.813, 0.942, 0.813, 0.788]
2024-07-01 08:27:55	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [460/500] Loss: 0.0004 Acc: [0.889, 0.957, 0.764, 0.813]
2024-07-01 08:28:14	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [470/500] Loss: 0.0001 Acc: [0.88, 0.957, 0.721, 0.798]
2024-07-01 08:28:32	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [480/500] Loss: 0.0002 Acc: [0.865, 0.947, 0.769, 0.846]
2024-07-01 08:28:51	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [490/500] Loss: 0.0002 Acc: [0.918, 0.938, 0.774, 0.798]
2024-07-01 08:29:09	 Model [<class 'layers.Model.Baseline_concat_mlp'>] Fold [4] Epoch [500/500] Loss: 0.0254 Acc: [0.856, 0.942, 0.822, 0.817]
mean acc: 0.75315124
mean f1: 0.7439056971721991
