Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='Ours data', data_path='ETTh1.csv', dec_in=7, depth=4, des='test', devices='0,1,2,3', dim=128, distil=True, dropout=0.1, e_layers=2, embed='timeF', enc_in=7, epochs=500, eval_save_frq=10, factor=1, features='M', freq='h', gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=1e-05, loss='MSE', lradj='type1', mask_rate=0.25, model='iTransformer', model_id='test', model_name='Baseline_SA', moving_avg=25, n_heads=8, num_class=4, num_kernels=6, num_workers=10, output_attention=True, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=3, pred_len=96, root_path='./data/ETT/', seasonal_patterns='Monthly', seq_len=300, target='OT', task_name='classification', tasks=[3, 3, 2, 4], top_k=5, train_epochs=10, use_amp=False, use_gpu=False, use_multi_gpu=False)

 <class 'layers.Model.Baseline_SA'>
Baseline_SA(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x SA_Blk(
      (tb): ModuleDict(
        (0): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
)
2024-07-01 08:29:50	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [010/500] Loss: 1.1164 Acc: [0.689, 0.579, 0.766, 0.383]
2024-07-01 08:30:23	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [020/500] Loss: 0.9204 Acc: [0.703, 0.641, 0.775, 0.445]
2024-07-01 08:30:56	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [030/500] Loss: 0.7896 Acc: [0.713, 0.665, 0.727, 0.507]
2024-07-01 08:31:28	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [040/500] Loss: 0.7114 Acc: [0.732, 0.665, 0.703, 0.545]
2024-07-01 08:32:01	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [050/500] Loss: 0.5947 Acc: [0.727, 0.689, 0.756, 0.646]
2024-07-01 08:32:33	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [060/500] Loss: 0.5320 Acc: [0.727, 0.756, 0.751, 0.603]
2024-07-01 08:33:06	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [070/500] Loss: 0.4332 Acc: [0.751, 0.813, 0.766, 0.665]
2024-07-01 08:33:39	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [080/500] Loss: 0.4237 Acc: [0.708, 0.799, 0.761, 0.555]
2024-07-01 08:34:11	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [090/500] Loss: 0.3660 Acc: [0.804, 0.866, 0.818, 0.665]
2024-07-01 08:34:44	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [100/500] Loss: 0.2575 Acc: [0.828, 0.88, 0.828, 0.66]
2024-07-01 08:35:16	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [110/500] Loss: 0.2657 Acc: [0.837, 0.88, 0.785, 0.689]
2024-07-01 08:35:49	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [120/500] Loss: 0.1485 Acc: [0.837, 0.885, 0.823, 0.703]
2024-07-01 08:36:22	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [130/500] Loss: 0.0975 Acc: [0.89, 0.9, 0.837, 0.713]
2024-07-01 08:36:54	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [140/500] Loss: 0.0786 Acc: [0.818, 0.923, 0.818, 0.694]
2024-07-01 08:37:27	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [150/500] Loss: 0.0676 Acc: [0.876, 0.947, 0.904, 0.77]
2024-07-01 08:37:59	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [160/500] Loss: 0.0415 Acc: [0.9, 0.962, 0.914, 0.794]
2024-07-01 08:38:32	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [170/500] Loss: 0.0191 Acc: [0.89, 0.938, 0.895, 0.756]
2024-07-01 08:39:05	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [180/500] Loss: 0.0242 Acc: [0.919, 0.967, 0.89, 0.742]
2024-07-01 08:39:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [190/500] Loss: 0.0084 Acc: [0.923, 0.957, 0.909, 0.809]
2024-07-01 08:40:10	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [200/500] Loss: 0.0121 Acc: [0.928, 0.952, 0.943, 0.809]
2024-07-01 08:40:42	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [210/500] Loss: 0.0053 Acc: [0.933, 0.967, 0.909, 0.775]
2024-07-01 08:41:15	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [220/500] Loss: 0.0058 Acc: [0.938, 0.971, 0.943, 0.799]
2024-07-01 08:41:47	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [230/500] Loss: 0.0053 Acc: [0.952, 0.976, 0.943, 0.837]
2024-07-01 08:42:20	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [240/500] Loss: 0.0033 Acc: [0.938, 0.938, 0.938, 0.809]
2024-07-01 08:42:53	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [250/500] Loss: 0.0421 Acc: [0.947, 0.943, 0.919, 0.833]
2024-07-01 08:43:25	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [260/500] Loss: 0.0021 Acc: [0.943, 0.962, 0.967, 0.856]
2024-07-01 08:43:58	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [270/500] Loss: 0.0060 Acc: [0.947, 0.957, 0.938, 0.828]
2024-07-01 08:44:30	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [280/500] Loss: 0.0014 Acc: [0.933, 0.967, 0.938, 0.861]
2024-07-01 08:45:03	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [290/500] Loss: 0.0006 Acc: [0.952, 0.957, 0.938, 0.823]
2024-07-01 08:45:36	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [300/500] Loss: 0.0013 Acc: [0.947, 0.971, 0.976, 0.861]
2024-07-01 08:46:08	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [310/500] Loss: 0.0007 Acc: [0.967, 0.962, 0.962, 0.88]
2024-07-01 08:46:41	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [320/500] Loss: 0.0017 Acc: [0.967, 0.962, 0.947, 0.842]
2024-07-01 08:47:14	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [330/500] Loss: 0.0008 Acc: [0.967, 0.962, 0.981, 0.847]
2024-07-01 08:47:46	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [340/500] Loss: 0.0002 Acc: [0.957, 0.962, 0.952, 0.866]
2024-07-01 08:48:19	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [350/500] Loss: 0.0001 Acc: [0.962, 0.962, 0.952, 0.871]
2024-07-01 08:48:51	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [360/500] Loss: 0.0008 Acc: [0.962, 0.943, 0.914, 0.876]
2024-07-01 08:49:24	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [370/500] Loss: 0.0006 Acc: [0.962, 0.962, 0.933, 0.871]
2024-07-01 08:49:57	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [380/500] Loss: 0.0003 Acc: [0.957, 0.967, 0.971, 0.89]
2024-07-01 08:50:29	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [390/500] Loss: 0.0002 Acc: [0.967, 0.962, 0.981, 0.876]
2024-07-01 08:51:02	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [400/500] Loss: 0.0004 Acc: [0.971, 0.962, 0.952, 0.88]
2024-07-01 08:51:34	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [410/500] Loss: 0.0059 Acc: [0.967, 0.852, 0.957, 0.885]
2024-07-01 08:52:07	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [420/500] Loss: 0.0003 Acc: [0.967, 0.971, 0.971, 0.842]
2024-07-01 08:52:40	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [430/500] Loss: 0.0001 Acc: [0.971, 0.971, 0.971, 0.904]
2024-07-01 08:53:12	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [440/500] Loss: 0.0000 Acc: [0.962, 0.962, 0.986, 0.861]
2024-07-01 08:53:45	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [450/500] Loss: 0.0008 Acc: [0.933, 0.861, 0.914, 0.828]
2024-07-01 08:54:17	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [460/500] Loss: 0.0004 Acc: [0.976, 0.967, 0.981, 0.909]
2024-07-01 08:54:50	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [470/500] Loss: 0.0009 Acc: [0.971, 0.971, 0.976, 0.885]
2024-07-01 08:55:23	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [480/500] Loss: 0.0001 Acc: [0.962, 0.967, 0.981, 0.885]
2024-07-01 08:55:55	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [490/500] Loss: 0.0001 Acc: [0.943, 0.962, 0.986, 0.89]
2024-07-01 08:56:28	 Model [<class 'layers.Model.Baseline_SA'>] Fold [0] Epoch [500/500] Loss: 0.0000 Acc: [0.962, 0.976, 0.981, 0.9]
mean acc: 0.7784689
mean f1: 0.7785994362530343
Baseline_SA(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x SA_Blk(
      (tb): ModuleDict(
        (0): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
)
2024-07-01 08:57:01	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [010/500] Loss: 1.1267 Acc: [0.641, 0.55, 0.789, 0.421]
2024-07-01 08:57:34	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [020/500] Loss: 0.9673 Acc: [0.646, 0.574, 0.789, 0.493]
2024-07-01 08:58:06	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [030/500] Loss: 0.8346 Acc: [0.651, 0.617, 0.799, 0.569]
2024-07-01 08:58:39	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [040/500] Loss: 0.7325 Acc: [0.651, 0.641, 0.756, 0.469]
2024-07-01 08:59:12	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [050/500] Loss: 0.7148 Acc: [0.679, 0.766, 0.703, 0.593]
2024-07-01 08:59:44	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [060/500] Loss: 0.6632 Acc: [0.684, 0.809, 0.727, 0.636]
2024-07-01 09:00:17	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [070/500] Loss: 0.5093 Acc: [0.675, 0.828, 0.732, 0.675]
2024-07-01 09:00:49	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [080/500] Loss: 0.4784 Acc: [0.708, 0.852, 0.751, 0.646]
2024-07-01 09:01:22	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [090/500] Loss: 0.3328 Acc: [0.742, 0.852, 0.632, 0.656]
2024-07-01 09:01:55	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [100/500] Loss: 0.2655 Acc: [0.799, 0.885, 0.713, 0.708]
2024-07-01 09:02:27	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [110/500] Loss: 0.2152 Acc: [0.813, 0.885, 0.842, 0.66]
2024-07-01 09:03:00	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [120/500] Loss: 0.2098 Acc: [0.837, 0.89, 0.852, 0.737]
2024-07-01 09:03:33	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [130/500] Loss: 0.1754 Acc: [0.842, 0.895, 0.88, 0.727]
2024-07-01 09:04:05	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [140/500] Loss: 0.1091 Acc: [0.856, 0.928, 0.928, 0.699]
2024-07-01 09:04:38	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [150/500] Loss: 0.0610 Acc: [0.9, 0.923, 0.895, 0.742]
2024-07-01 09:05:11	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [160/500] Loss: 0.0591 Acc: [0.904, 0.923, 0.909, 0.818]
2024-07-01 09:05:43	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [170/500] Loss: 0.0271 Acc: [0.904, 0.938, 0.89, 0.818]
2024-07-01 09:06:16	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [180/500] Loss: 0.0165 Acc: [0.923, 0.919, 0.943, 0.789]
2024-07-01 09:06:48	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [190/500] Loss: 0.0297 Acc: [0.909, 0.9, 0.928, 0.718]
2024-07-01 09:07:21	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [200/500] Loss: 0.0224 Acc: [0.919, 0.947, 0.933, 0.789]
2024-07-01 09:07:54	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [210/500] Loss: 0.0155 Acc: [0.919, 0.952, 0.89, 0.866]
2024-07-01 09:08:26	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [220/500] Loss: 0.0048 Acc: [0.909, 0.967, 0.833, 0.799]
2024-07-01 09:08:59	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [230/500] Loss: 0.0046 Acc: [0.914, 0.952, 0.919, 0.842]
2024-07-01 09:09:31	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [240/500] Loss: 0.0041 Acc: [0.938, 0.952, 0.89, 0.833]
2024-07-01 09:10:04	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [250/500] Loss: 0.0067 Acc: [0.928, 0.928, 0.957, 0.785]
2024-07-01 09:10:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [260/500] Loss: 0.0024 Acc: [0.938, 0.957, 0.914, 0.856]
2024-07-01 09:11:09	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [270/500] Loss: 0.0018 Acc: [0.9, 0.962, 0.957, 0.823]
2024-07-01 09:11:42	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [280/500] Loss: 0.0017 Acc: [0.943, 0.971, 0.919, 0.856]
2024-07-01 09:12:14	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [290/500] Loss: 0.0062 Acc: [0.928, 0.943, 0.909, 0.866]
2024-07-01 09:12:47	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [300/500] Loss: 0.0018 Acc: [0.947, 0.962, 0.852, 0.809]
2024-07-01 09:13:20	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [310/500] Loss: 0.0034 Acc: [0.947, 0.99, 0.904, 0.885]
2024-07-01 09:13:52	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [320/500] Loss: 0.0006 Acc: [0.957, 0.976, 0.9, 0.89]
2024-07-01 09:14:25	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [330/500] Loss: 0.0007 Acc: [0.962, 0.986, 0.981, 0.9]
2024-07-01 09:14:57	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [340/500] Loss: 0.0019 Acc: [0.962, 0.986, 0.986, 0.895]
2024-07-01 09:15:30	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [350/500] Loss: 0.0010 Acc: [0.976, 0.99, 0.981, 0.914]
2024-07-01 09:16:03	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [360/500] Loss: 0.0006 Acc: [0.957, 0.976, 0.986, 0.876]
2024-07-01 09:16:35	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [370/500] Loss: 0.0003 Acc: [0.976, 0.981, 0.981, 0.904]
2024-07-01 09:17:08	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [380/500] Loss: 0.0006 Acc: [0.981, 0.976, 0.962, 0.909]
2024-07-01 09:17:40	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [390/500] Loss: 0.0023 Acc: [0.976, 0.976, 0.981, 0.895]
2024-07-01 09:18:13	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [400/500] Loss: 0.0008 Acc: [0.971, 0.99, 0.981, 0.895]
2024-07-01 09:18:45	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [410/500] Loss: 0.0005 Acc: [0.962, 0.981, 0.962, 0.88]
2024-07-01 09:19:18	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [420/500] Loss: 0.2080 Acc: [0.847, 0.856, 0.775, 0.78]
2024-07-01 09:19:51	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [430/500] Loss: 0.0004 Acc: [0.971, 0.981, 0.986, 0.904]
2024-07-01 09:20:23	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [440/500] Loss: 0.0004 Acc: [0.952, 0.99, 0.981, 0.861]
2024-07-01 09:20:56	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [450/500] Loss: 0.0001 Acc: [0.971, 0.99, 0.981, 0.885]
2024-07-01 09:21:28	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [460/500] Loss: 0.0001 Acc: [0.971, 0.99, 0.986, 0.847]
2024-07-01 09:22:01	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [470/500] Loss: 0.0016 Acc: [0.957, 0.986, 0.957, 0.866]
2024-07-01 09:22:33	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [480/500] Loss: 0.0001 Acc: [0.976, 0.99, 0.981, 0.861]
2024-07-01 09:23:06	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [490/500] Loss: 0.0002 Acc: [0.971, 0.986, 0.981, 0.866]
2024-07-01 09:23:39	 Model [<class 'layers.Model.Baseline_SA'>] Fold [1] Epoch [500/500] Loss: 0.0007 Acc: [0.904, 0.99, 0.818, 0.871]
mean acc: 0.78205734
mean f1: 0.7809578861693767
Baseline_SA(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x SA_Blk(
      (tb): ModuleDict(
        (0): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
)
2024-07-01 09:24:12	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [010/500] Loss: 1.0997 Acc: [0.646, 0.699, 0.718, 0.392]
2024-07-01 09:24:45	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [020/500] Loss: 0.9434 Acc: [0.66, 0.708, 0.732, 0.421]
2024-07-01 09:25:17	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [030/500] Loss: 0.8335 Acc: [0.66, 0.727, 0.746, 0.56]
2024-07-01 09:25:50	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [040/500] Loss: 0.6602 Acc: [0.689, 0.77, 0.775, 0.545]
2024-07-01 09:26:22	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [050/500] Loss: 0.5702 Acc: [0.713, 0.722, 0.751, 0.589]
2024-07-01 09:26:55	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [060/500] Loss: 0.5677 Acc: [0.722, 0.761, 0.794, 0.622]
2024-07-01 09:27:28	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [070/500] Loss: 0.4946 Acc: [0.722, 0.828, 0.789, 0.651]
2024-07-01 09:28:00	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [080/500] Loss: 0.3860 Acc: [0.722, 0.799, 0.813, 0.636]
2024-07-01 09:28:33	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [090/500] Loss: 0.3430 Acc: [0.727, 0.828, 0.842, 0.598]
2024-07-01 09:29:05	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [100/500] Loss: 0.2322 Acc: [0.727, 0.833, 0.78, 0.603]
2024-07-01 09:29:38	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [110/500] Loss: 0.2221 Acc: [0.761, 0.804, 0.847, 0.708]
2024-07-01 09:30:11	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [120/500] Loss: 0.1720 Acc: [0.785, 0.828, 0.818, 0.722]
2024-07-01 09:30:43	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [130/500] Loss: 0.1297 Acc: [0.809, 0.861, 0.799, 0.665]
2024-07-01 09:31:16	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [140/500] Loss: 0.0951 Acc: [0.732, 0.842, 0.818, 0.722]
2024-07-01 09:31:48	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [150/500] Loss: 0.3077 Acc: [0.804, 0.9, 0.852, 0.727]
2024-07-01 09:32:21	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [160/500] Loss: 0.0549 Acc: [0.828, 0.919, 0.938, 0.789]
2024-07-01 09:32:54	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [170/500] Loss: 0.1133 Acc: [0.856, 0.938, 0.909, 0.837]
2024-07-01 09:33:26	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [180/500] Loss: 0.0168 Acc: [0.852, 0.876, 0.78, 0.78]
2024-07-01 09:33:59	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [190/500] Loss: 0.0131 Acc: [0.885, 0.923, 0.895, 0.789]
2024-07-01 09:34:32	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [200/500] Loss: 0.0093 Acc: [0.89, 0.967, 0.88, 0.77]
2024-07-01 09:35:04	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [210/500] Loss: 0.0133 Acc: [0.9, 0.909, 0.852, 0.775]
2024-07-01 09:35:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [220/500] Loss: 0.0088 Acc: [0.9, 0.933, 0.923, 0.876]
2024-07-01 09:36:10	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [230/500] Loss: 0.0571 Acc: [0.923, 0.895, 0.871, 0.794]
2024-07-01 09:36:42	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [240/500] Loss: 0.0082 Acc: [0.923, 0.957, 0.923, 0.876]
2024-07-01 09:37:15	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [250/500] Loss: 0.0021 Acc: [0.933, 0.947, 0.938, 0.89]
2024-07-01 09:37:47	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [260/500] Loss: 0.0025 Acc: [0.952, 0.957, 0.947, 0.904]
2024-07-01 09:38:20	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [270/500] Loss: 0.0084 Acc: [0.947, 0.909, 0.943, 0.785]
2024-07-01 09:38:53	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [280/500] Loss: 0.0016 Acc: [0.957, 0.947, 0.938, 0.909]
2024-07-01 09:39:25	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [290/500] Loss: 0.0015 Acc: [0.957, 0.943, 0.962, 0.876]
2024-07-01 09:39:58	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [300/500] Loss: 0.0013 Acc: [0.957, 0.947, 0.923, 0.89]
2024-07-01 09:40:31	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [310/500] Loss: 0.0003 Acc: [0.957, 0.938, 0.923, 0.9]
2024-07-01 09:41:03	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [320/500] Loss: 0.0009 Acc: [0.947, 0.928, 0.943, 0.9]
2024-07-01 09:41:36	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [330/500] Loss: 0.0009 Acc: [0.947, 0.952, 0.904, 0.871]
2024-07-01 09:42:09	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [340/500] Loss: 0.0025 Acc: [0.952, 0.943, 0.957, 0.904]
2024-07-01 09:42:41	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [350/500] Loss: 0.0011 Acc: [0.962, 0.962, 0.971, 0.928]
2024-07-01 09:43:14	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [360/500] Loss: 0.0006 Acc: [0.962, 0.976, 0.995, 0.914]
2024-07-01 09:43:46	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [370/500] Loss: 0.0025 Acc: [0.962, 0.981, 0.981, 0.919]
2024-07-01 09:44:19	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [380/500] Loss: 0.0004 Acc: [0.967, 0.957, 0.986, 0.914]
2024-07-01 09:44:52	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [390/500] Loss: 0.0005 Acc: [0.971, 0.928, 0.876, 0.914]
2024-07-01 09:45:24	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [400/500] Loss: 0.0002 Acc: [0.967, 0.947, 0.967, 0.933]
2024-07-01 09:45:57	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [410/500] Loss: 0.0001 Acc: [0.962, 0.962, 0.962, 0.909]
2024-07-01 09:46:30	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [420/500] Loss: 0.0002 Acc: [0.967, 0.971, 0.99, 0.904]
2024-07-01 09:47:02	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [430/500] Loss: 0.0000 Acc: [0.962, 0.971, 0.995, 0.909]
2024-07-01 09:47:35	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [440/500] Loss: 0.0014 Acc: [0.971, 0.971, 0.99, 0.928]
2024-07-01 09:48:07	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [450/500] Loss: 0.0007 Acc: [0.967, 0.971, 0.933, 0.923]
2024-07-01 09:48:40	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [460/500] Loss: 0.0002 Acc: [0.971, 0.971, 0.981, 0.919]
2024-07-01 09:49:13	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [470/500] Loss: 0.0002 Acc: [0.967, 0.943, 0.986, 0.923]
2024-07-01 09:49:45	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [480/500] Loss: 0.0004 Acc: [0.971, 0.986, 0.99, 0.89]
2024-07-01 09:50:18	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [490/500] Loss: 0.0001 Acc: [0.971, 0.981, 0.99, 0.919]
2024-07-01 09:50:51	 Model [<class 'layers.Model.Baseline_SA'>] Fold [2] Epoch [500/500] Loss: 0.0005 Acc: [0.962, 0.861, 0.89, 0.914]
mean acc: 0.7876236
mean f1: 0.786084387201237
Baseline_SA(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x SA_Blk(
      (tb): ModuleDict(
        (0): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
)
2024-07-01 09:51:24	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [010/500] Loss: 1.0818 Acc: [0.684, 0.536, 0.756, 0.378]
2024-07-01 09:51:57	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [020/500] Loss: 0.9123 Acc: [0.694, 0.689, 0.766, 0.502]
2024-07-01 09:52:29	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [030/500] Loss: 0.8248 Acc: [0.708, 0.708, 0.766, 0.483]
2024-07-01 09:53:02	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [040/500] Loss: 0.7296 Acc: [0.746, 0.713, 0.746, 0.522]
2024-07-01 09:53:34	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [050/500] Loss: 0.6711 Acc: [0.737, 0.699, 0.742, 0.593]
2024-07-01 09:54:07	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [060/500] Loss: 0.7343 Acc: [0.722, 0.756, 0.718, 0.598]
2024-07-01 09:54:39	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [070/500] Loss: 0.5487 Acc: [0.722, 0.818, 0.756, 0.589]
2024-07-01 09:55:12	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [080/500] Loss: 0.5491 Acc: [0.737, 0.818, 0.813, 0.636]
2024-07-01 09:55:45	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [090/500] Loss: 0.4430 Acc: [0.77, 0.789, 0.856, 0.612]
2024-07-01 09:56:17	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [100/500] Loss: 0.3904 Acc: [0.785, 0.789, 0.703, 0.651]
2024-07-01 09:56:50	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [110/500] Loss: 0.3195 Acc: [0.804, 0.866, 0.746, 0.646]
2024-07-01 09:57:22	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [120/500] Loss: 0.1927 Acc: [0.847, 0.775, 0.785, 0.766]
2024-07-01 09:57:55	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [130/500] Loss: 0.1245 Acc: [0.847, 0.818, 0.732, 0.756]
2024-07-01 09:58:28	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [140/500] Loss: 0.1497 Acc: [0.852, 0.837, 0.88, 0.751]
2024-07-01 09:59:00	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [150/500] Loss: 0.0761 Acc: [0.837, 0.876, 0.909, 0.713]
2024-07-01 09:59:33	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [160/500] Loss: 0.0468 Acc: [0.895, 0.837, 0.943, 0.766]
2024-07-01 10:00:06	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [170/500] Loss: 0.0291 Acc: [0.89, 0.852, 0.919, 0.794]
2024-07-01 10:00:38	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [180/500] Loss: 0.0250 Acc: [0.909, 0.904, 0.799, 0.766]
2024-07-01 10:01:11	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [190/500] Loss: 0.0455 Acc: [0.909, 0.909, 0.967, 0.789]
2024-07-01 10:01:44	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [200/500] Loss: 0.0229 Acc: [0.923, 0.866, 0.938, 0.856]
2024-07-01 10:02:16	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [210/500] Loss: 0.0279 Acc: [0.933, 0.895, 0.943, 0.847]
2024-07-01 10:02:49	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [220/500] Loss: 0.0110 Acc: [0.943, 0.895, 0.923, 0.847]
2024-07-01 10:03:21	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [230/500] Loss: 0.0096 Acc: [0.928, 0.895, 0.947, 0.866]
2024-07-01 10:03:54	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [240/500] Loss: 0.0058 Acc: [0.947, 0.9, 0.976, 0.895]
2024-07-01 10:04:27	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [250/500] Loss: 0.0033 Acc: [0.938, 0.89, 0.947, 0.9]
2024-07-01 10:04:59	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [260/500] Loss: 0.0175 Acc: [0.957, 0.933, 0.976, 0.9]
2024-07-01 10:05:32	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [270/500] Loss: 0.0093 Acc: [0.962, 0.914, 0.976, 0.9]
2024-07-01 10:06:05	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [280/500] Loss: 0.0050 Acc: [0.967, 0.904, 0.976, 0.919]
2024-07-01 10:06:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [290/500] Loss: 0.0021 Acc: [0.962, 0.919, 0.986, 0.909]
2024-07-01 10:07:10	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [300/500] Loss: 0.0128 Acc: [0.967, 0.909, 0.856, 0.856]
2024-07-01 10:07:42	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [310/500] Loss: 0.0009 Acc: [0.967, 0.909, 0.986, 0.909]
2024-07-01 10:08:15	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [320/500] Loss: 0.0032 Acc: [0.967, 0.933, 0.981, 0.914]
2024-07-01 10:08:48	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [330/500] Loss: 0.0014 Acc: [0.971, 0.914, 0.976, 0.928]
2024-07-01 10:09:20	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [340/500] Loss: 0.0020 Acc: [0.962, 0.914, 0.976, 0.943]
2024-07-01 10:09:53	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [350/500] Loss: 0.0018 Acc: [0.957, 0.928, 0.976, 0.919]
2024-07-01 10:10:26	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [360/500] Loss: 0.0013 Acc: [0.981, 0.928, 0.99, 0.923]
2024-07-01 10:10:59	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [370/500] Loss: 0.0003 Acc: [0.971, 0.914, 0.986, 0.909]
2024-07-01 10:11:31	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [380/500] Loss: 0.0003 Acc: [0.981, 0.923, 0.976, 0.919]
2024-07-01 10:12:04	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [390/500] Loss: 0.0007 Acc: [0.971, 0.914, 0.986, 0.923]
2024-07-01 10:12:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [400/500] Loss: 0.0004 Acc: [0.986, 0.919, 0.986, 0.904]
2024-07-01 10:13:09	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [410/500] Loss: 0.0002 Acc: [0.986, 0.933, 0.99, 0.923]
2024-07-01 10:13:42	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [420/500] Loss: 0.0003 Acc: [0.971, 0.928, 0.99, 0.943]
2024-07-01 10:14:15	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [430/500] Loss: 0.0016 Acc: [0.976, 0.943, 0.981, 0.919]
2024-07-01 10:14:47	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [440/500] Loss: 0.0016 Acc: [0.981, 0.938, 0.976, 0.919]
2024-07-01 10:15:20	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [450/500] Loss: 0.0008 Acc: [0.981, 0.914, 0.986, 0.923]
2024-07-01 10:15:53	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [460/500] Loss: 0.0020 Acc: [0.981, 0.914, 0.986, 0.923]
2024-07-01 10:16:25	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [470/500] Loss: 0.0003 Acc: [0.971, 0.947, 0.99, 0.933]
2024-07-01 10:16:58	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [480/500] Loss: 0.0009 Acc: [0.981, 0.957, 0.986, 0.933]
2024-07-01 10:17:31	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [490/500] Loss: 0.0049 Acc: [0.99, 0.933, 0.986, 0.947]
2024-07-01 10:18:03	 Model [<class 'layers.Model.Baseline_SA'>] Fold [3] Epoch [500/500] Loss: 0.0006 Acc: [0.947, 0.947, 0.99, 0.928]
mean acc: 0.7931578
mean f1: 0.79208432821307
Baseline_SA(
  (enc_embedding): DataEmbedding_inverted(
    (value_embedding): Linear(in_features=300, out_features=512, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): Encoder(
    (attn_layers): ModuleList(
      (0-1): 2 x EncoderLayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=512, out_features=512, bias=True)
          (key_projection): Linear(in_features=512, out_features=512, bias=True)
          (value_projection): Linear(in_features=512, out_features=512, bias=True)
          (out_projection): Linear(in_features=512, out_features=512, bias=True)
        )
        (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))
        (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (proj): Linear(in_features=512, out_features=128, bias=True)
  (cls_tokens): ParameterDict(
      (0): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (2): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (3): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
      (4): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128 (GPU 0)]
  )
  (task_heads): ModuleDict(
    (0): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (1): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=3, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (2): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=2, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (3): Mlp(
      (fc1): Linear(in_features=128, out_features=128, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=128, out_features=4, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
  )
  (our_blks): ModuleList(
    (0-3): 4 x SA_Blk(
      (tb): ModuleDict(
        (0): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (2): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): ModuleDict(
          (sa): SA(
            (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (sa): SelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=False)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=128, bias=True)
              (act): GELU(approximate='none')
              (fc2): Linear(in_features=128, out_features=128, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
  )
)
2024-07-01 10:18:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [010/500] Loss: 1.1594 Acc: [0.697, 0.606, 0.764, 0.438]
2024-07-01 10:19:10	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [020/500] Loss: 1.0070 Acc: [0.707, 0.63, 0.764, 0.5]
2024-07-01 10:19:43	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [030/500] Loss: 0.9171 Acc: [0.702, 0.644, 0.803, 0.49]
2024-07-01 10:20:15	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [040/500] Loss: 0.8913 Acc: [0.697, 0.644, 0.774, 0.519]
2024-07-01 10:20:48	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [050/500] Loss: 0.8462 Acc: [0.716, 0.712, 0.837, 0.606]
2024-07-01 10:21:21	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [060/500] Loss: 0.7012 Acc: [0.721, 0.702, 0.803, 0.572]
2024-07-01 10:21:53	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [070/500] Loss: 0.6701 Acc: [0.716, 0.726, 0.803, 0.611]
2024-07-01 10:22:26	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [080/500] Loss: 0.7028 Acc: [0.707, 0.889, 0.87, 0.601]
2024-07-01 10:22:59	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [090/500] Loss: 0.4490 Acc: [0.716, 0.837, 0.736, 0.659]
2024-07-01 10:23:32	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [100/500] Loss: 0.3000 Acc: [0.721, 0.774, 0.644, 0.577]
2024-07-01 10:24:04	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [110/500] Loss: 0.2818 Acc: [0.736, 0.827, 0.635, 0.75]
2024-07-01 10:24:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [120/500] Loss: 0.2268 Acc: [0.74, 0.861, 0.721, 0.76]
2024-07-01 10:25:10	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [130/500] Loss: 0.1167 Acc: [0.841, 0.798, 0.692, 0.74]
2024-07-01 10:25:43	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [140/500] Loss: 0.1113 Acc: [0.851, 0.779, 0.822, 0.601]
2024-07-01 10:26:15	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [150/500] Loss: 0.0840 Acc: [0.913, 0.832, 0.808, 0.827]
2024-07-01 10:26:48	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [160/500] Loss: 0.1447 Acc: [0.909, 0.851, 0.841, 0.813]
2024-07-01 10:27:21	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [170/500] Loss: 0.0480 Acc: [0.909, 0.885, 0.88, 0.822]
2024-07-01 10:27:54	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [180/500] Loss: 0.0268 Acc: [0.933, 0.822, 0.846, 0.837]
2024-07-01 10:28:26	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [190/500] Loss: 0.0250 Acc: [0.909, 0.909, 0.904, 0.793]
2024-07-01 10:28:59	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [200/500] Loss: 0.0214 Acc: [0.923, 0.865, 0.856, 0.846]
2024-07-01 10:29:32	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [210/500] Loss: 0.0299 Acc: [0.933, 0.865, 0.856, 0.88]
2024-07-01 10:30:04	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [220/500] Loss: 0.0234 Acc: [0.928, 0.88, 0.899, 0.865]
2024-07-01 10:30:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [230/500] Loss: 0.0385 Acc: [0.923, 0.928, 0.928, 0.875]
2024-07-01 10:31:10	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [240/500] Loss: 0.0137 Acc: [0.938, 0.938, 0.899, 0.856]
2024-07-01 10:31:43	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [250/500] Loss: 0.0112 Acc: [0.933, 0.832, 0.904, 0.889]
2024-07-01 10:32:15	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [260/500] Loss: 0.0056 Acc: [0.957, 0.889, 0.928, 0.88]
2024-07-01 10:32:48	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [270/500] Loss: 0.0069 Acc: [0.942, 0.928, 0.933, 0.894]
2024-07-01 10:33:21	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [280/500] Loss: 0.0038 Acc: [0.942, 0.904, 0.865, 0.875]
2024-07-01 10:33:53	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [290/500] Loss: 0.0070 Acc: [0.952, 0.938, 0.913, 0.904]
2024-07-01 10:34:26	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [300/500] Loss: 0.0112 Acc: [0.952, 0.933, 0.904, 0.918]
2024-07-01 10:34:59	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [310/500] Loss: 0.0025 Acc: [0.966, 0.938, 0.938, 0.851]
2024-07-01 10:35:31	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [320/500] Loss: 0.0230 Acc: [0.966, 0.74, 0.952, 0.904]
2024-07-01 10:36:04	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [330/500] Loss: 0.0014 Acc: [0.957, 0.952, 0.933, 0.909]
2024-07-01 10:36:37	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [340/500] Loss: 0.0008 Acc: [0.971, 0.947, 0.947, 0.913]
2024-07-01 10:37:10	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [350/500] Loss: 0.0025 Acc: [0.976, 0.928, 0.918, 0.918]
2024-07-01 10:37:42	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [360/500] Loss: 0.0056 Acc: [0.976, 0.918, 0.952, 0.928]
2024-07-01 10:38:15	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [370/500] Loss: 0.0029 Acc: [0.976, 0.99, 0.913, 0.918]
2024-07-01 10:38:48	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [380/500] Loss: 0.0002 Acc: [0.976, 0.957, 0.938, 0.928]
2024-07-01 10:39:21	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [390/500] Loss: 0.4319 Acc: [0.966, 0.822, 0.726, 0.865]
2024-07-01 10:39:53	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [400/500] Loss: 0.0011 Acc: [0.966, 0.957, 0.947, 0.913]
2024-07-01 10:40:26	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [410/500] Loss: 0.0018 Acc: [0.966, 0.966, 0.952, 0.923]
2024-07-01 10:41:00	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [420/500] Loss: 0.0002 Acc: [0.966, 0.976, 0.952, 0.909]
2024-07-01 10:41:33	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [430/500] Loss: 0.0020 Acc: [0.966, 0.966, 0.942, 0.904]
2024-07-01 10:42:06	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [440/500] Loss: 0.0002 Acc: [0.966, 0.981, 0.942, 0.904]
2024-07-01 10:42:39	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [450/500] Loss: 0.0172 Acc: [0.986, 0.981, 0.88, 0.904]
2024-07-01 10:43:12	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [460/500] Loss: 0.0001 Acc: [0.971, 0.966, 0.957, 0.918]
2024-07-01 10:43:45	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [470/500] Loss: 0.0032 Acc: [0.971, 0.976, 0.952, 0.928]
2024-07-01 10:44:18	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [480/500] Loss: 0.0003 Acc: [0.966, 0.966, 0.966, 0.913]
2024-07-01 10:44:51	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [490/500] Loss: 0.0001 Acc: [0.981, 0.976, 0.957, 0.899]
2024-07-01 10:45:24	 Model [<class 'layers.Model.Baseline_SA'>] Fold [4] Epoch [500/500] Loss: 0.0002 Acc: [0.986, 0.981, 0.957, 0.928]
mean acc: 0.7960263
mean f1: 0.7946935783168806
